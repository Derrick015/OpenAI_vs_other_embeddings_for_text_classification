{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-3 vs Other Text Embeddings Techniques for Text Classification: A Performance Evaluation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Importation and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>combined</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B003XPF9BO</td>\n",
       "      <td>A3R7JR3FMEBXQB</td>\n",
       "      <td>5</td>\n",
       "      <td>where does one  start...and stop... with a tre...</td>\n",
       "      <td>Wanted to save some to bring to my Chicago fam...</td>\n",
       "      <td>Title: where does one  start...and stop... wit...</td>\n",
       "      <td>52</td>\n",
       "      <td>[0.007018072064965963, -0.02731654793024063, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>B003VXHGPK</td>\n",
       "      <td>A21VWSCGW7UUAR</td>\n",
       "      <td>4</td>\n",
       "      <td>Good, but not Wolfgang Puck good</td>\n",
       "      <td>Honestly, I have to admit that I expected a li...</td>\n",
       "      <td>Title: Good, but not Wolfgang Puck good; Conte...</td>\n",
       "      <td>178</td>\n",
       "      <td>[-0.003140551969408989, -0.009995664469897747,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>B008JKTTUA</td>\n",
       "      <td>A34XBAIFT02B60</td>\n",
       "      <td>1</td>\n",
       "      <td>Should advertise coconut as an ingredient more...</td>\n",
       "      <td>First, these should be called Mac - Coconut ba...</td>\n",
       "      <td>Title: Should advertise coconut as an ingredie...</td>\n",
       "      <td>78</td>\n",
       "      <td>[-0.01757248118519783, -8.266511576948687e-05,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ProductId          UserId  Score  \\\n",
       "0    B003XPF9BO  A3R7JR3FMEBXQB      5   \n",
       "297  B003VXHGPK  A21VWSCGW7UUAR      4   \n",
       "296  B008JKTTUA  A34XBAIFT02B60      1   \n",
       "\n",
       "                                               Summary  \\\n",
       "0    where does one  start...and stop... with a tre...   \n",
       "297                   Good, but not Wolfgang Puck good   \n",
       "296  Should advertise coconut as an ingredient more...   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    Wanted to save some to bring to my Chicago fam...   \n",
       "297  Honestly, I have to admit that I expected a li...   \n",
       "296  First, these should be called Mac - Coconut ba...   \n",
       "\n",
       "                                              combined  n_tokens  \\\n",
       "0    Title: where does one  start...and stop... wit...        52   \n",
       "297  Title: Good, but not Wolfgang Puck good; Conte...       178   \n",
       "296  Title: Should advertise coconut as an ingredie...        78   \n",
       "\n",
       "                                             embedding  \n",
       "0    [0.007018072064965963, -0.02731654793024063, 0...  \n",
       "297  [-0.003140551969408989, -0.009995664469897747,...  \n",
       "296  [-0.01757248118519783, -8.266511576948687e-05,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import gensim.downloader as api\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import re\n",
    "\n",
    "# import data \n",
    "df1 = pd.read_csv('https://raw.githubusercontent.com/openai/openai-cookbook/main/examples/data/fine_food_reviews_with_embeddings_1k.csv',\n",
    "                  index_col=0)\n",
    "\n",
    "# view first three rows\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean openai embeddings\n",
    "def clean_emb(text):\n",
    "\n",
    "# remove line break\n",
    "    text = re.sub(r'\\n', '', text) \n",
    "\n",
    "# remove square brackets\n",
    "    text = re.sub(r'\\[|\\]', \"\", text)\n",
    "\n",
    "# remove leading and trailing white spaces\n",
    "    text = text.strip() \n",
    "\n",
    "# convert string into array\n",
    "    text = np.fromstring(text, dtype=float, sep=',') \n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Rename column to gpt_3\n",
    "df1.rename(columns={'embedding': 'gpt_3'}, inplace=True) \n",
    "\n",
    "# Apply clean_emb function\n",
    "df1['gpt_3'] = df1['gpt_3'].apply(lambda x: clean_emb(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embeddig Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. GPT-3 Embeddigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'Enter api key here'\n",
    "\n",
    "# set api key as default api key for openai\n",
    "openai.api_key = api_key\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "\n",
    "# replace new lines with spaces\n",
    "   text = text.replace(\"\\n\", \" \") \n",
    "\n",
    "# openai.Embedding.create to convert text into embedding array\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 GloVe Embeddigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in terminal first: python -m spacy download en_core_web_lg\n",
    "# ! pip install spacy\n",
    "import spacy\n",
    "\n",
    "# load pipeline\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: where does one  start...and stop... with a treat like this; Content: Wanted to save some to bring to my Chicago family but my North Carolina family ate all 4 boxes before I could pack. These are excellent...could serve to anyone'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first text input\n",
    "df1.combined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_multiple_fullstops(text):\n",
    "\n",
    "# replace 2 or more consecutive fullstops with 1\n",
    "     text = re.sub(r'\\.{2,}', '.', text) \n",
    "\n",
    "# strip white spaces from ends of sentence\n",
    "     text= text.strip() \n",
    "\n",
    "     return text\n",
    "\n",
    "# Apply function \n",
    "df1['clean_text'] = df1['combined'].apply(lambda x: replace_multiple_fullstops(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding vectors in a variable called glove\n",
    "df1['glove'] = df1['clean_text'].apply(lambda text: nlp(text).vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Word2vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load word2vec-google-news-300 model\n",
    "wv = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wv_preprocess_and_vectorize(text):\n",
    "    # Process the input text using a natural language processing library\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize a list to store the filtered tokens\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    # Loop through each token in the doc\n",
    "    for token in doc:\n",
    "        # If the token is a stop word or punctuation, skip it\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        # Otherwise, add the lemma of the token to the filtered_tokens list\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    \n",
    "    # If there are no filtered tokens, return np.nan\n",
    "    if not filtered_tokens:\n",
    "        return np.nan\n",
    "    else:\n",
    "        # Otherwise, return the mean vector representation of the filtered tokens\n",
    "        return wv.get_mean_vector(filtered_tokens)\n",
    "\n",
    "# Apply function\n",
    "df1['word2vec'] = df1['clean_text'].apply(lambda text: wv_preprocess_and_vectorize(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 MPNet Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all-mpnet-base-v2 model\n",
    "model_sent = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Apply model\n",
    "df1['mpnet'] = df1['clean_text'].apply(lambda text: model_sent.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dimensionality Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c6cbf_row0_col1 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c6cbf_row1_col1 {\n",
       "  background-color: #a5bddb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c6cbf_row2_col1, #T_c6cbf_row3_col1 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c6cbf\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c6cbf_level0_col0\" class=\"col_heading level0 col0\" >Name</th>\n",
       "      <th id=\"T_c6cbf_level0_col1\" class=\"col_heading level0 col1\" >Dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c6cbf_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c6cbf_row0_col0\" class=\"data row0 col0\" >gpt_3</td>\n",
       "      <td id=\"T_c6cbf_row0_col1\" class=\"data row0 col1\" >1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c6cbf_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c6cbf_row1_col0\" class=\"data row1 col0\" >mpnet</td>\n",
       "      <td id=\"T_c6cbf_row1_col1\" class=\"data row1 col1\" >768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c6cbf_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c6cbf_row2_col0\" class=\"data row2 col0\" >word2vec</td>\n",
       "      <td id=\"T_c6cbf_row2_col1\" class=\"data row2 col1\" >300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c6cbf_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c6cbf_row3_col0\" class=\"data row3 col0\" >glove</td>\n",
       "      <td id=\"T_c6cbf_row3_col1\" class=\"data row3 col1\" >300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x174d9ccead0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign data of lists.  \n",
    "data = {'Name': ['gpt_3', 'mpnet', 'word2vec', 'glove'],\n",
    "         'Dimension': [len(df1.gpt_3[0]), len(df1.mpnet[0]), \n",
    "                        len(df1.word2vec[0]), len(df1.glove[0])]}  \n",
    "  \n",
    "# Create DataFrame  \n",
    "df_emb_len = pd.DataFrame(data)  \n",
    "\n",
    "# Set background style\n",
    "df_emb_len.style.background_gradient()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of embedding methods to evaluate\n",
    "embedding_var= ['gpt_3', 'mpnet', 'word2vec', 'glove']\n",
    "\n",
    "# Define a list of classifier models to use\n",
    "classifiers = [('rf', RandomForestClassifier(random_state=76)),\n",
    "                ('svm', SVC(random_state=76)), \n",
    "                ('lr', LogisticRegression(random_state=76, max_iter=400)),\n",
    "                ('dt', DecisionTreeClassifier(random_state=76))]\n",
    "\n",
    "# Define a dictionary to store accuracy results for each classifier\n",
    "accuracy_lists = {\n",
    "    'rf': [],\n",
    "    'svm': [],\n",
    "    'lr': [],\n",
    "    'dt': []\n",
    "}\n",
    "\n",
    "# Loop through each embedding method\n",
    "for emb in embedding_var:\n",
    "\n",
    "    # Split the data into training and testing sets using the 'train_test_split' function\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df1[emb].values,\n",
    "        df1.Score,\n",
    "        test_size=0.25,\n",
    "        random_state=76\n",
    "    )\n",
    "\n",
    "    # Stack the training and testing sets into 3D arrays\n",
    "    X_train_stacked = np.stack(X_train)\n",
    "    X_test_stacked = np.stack(X_test)\n",
    "\n",
    "    # Loop through each classifier model\n",
    "    for classifier_name, classifier in classifiers:\n",
    "\n",
    "        # Create a pipeline that scales the data and fits the classifier\n",
    "        pipe = Pipeline([('scaler', RobustScaler()), (classifier_name, classifier)])\n",
    "        pipe.fit(X_train_stacked, y_train)\n",
    "\n",
    "        # Use the pipeline to make predictions on the test data\n",
    "        y_pred = pipe.predict(X_test_stacked)\n",
    "\n",
    "        # Evaluate the accuracy of the predictions\n",
    "        report = classification_report(y_test, y_pred ,output_dict=True)\n",
    "        acc = report['accuracy']\n",
    "\n",
    "        # Store the accuracy results for each classifier\n",
    "        accuracy_lists[classifier_name].append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d4a46_row0_col1, #T_d4a46_row0_col2, #T_d4a46_row0_col3, #T_d4a46_row0_col4 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d4a46_row1_col1 {\n",
       "  background-color: #93b5d6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d4a46_row1_col2 {\n",
       "  background-color: #c4cbe3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d4a46_row1_col3, #T_d4a46_row1_col4, #T_d4a46_row3_col1, #T_d4a46_row3_col2, #T_d4a46_row3_col3 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d4a46_row2_col1 {\n",
       "  background-color: #c9cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d4a46_row2_col2 {\n",
       "  background-color: #e8e4f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d4a46_row2_col3 {\n",
       "  background-color: #d0d1e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_d4a46_row2_col4 {\n",
       "  background-color: #2f8bbe;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_d4a46_row3_col4 {\n",
       "  background-color: #045d92;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d4a46\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d4a46_level0_col0\" class=\"col_heading level0 col0\" >Embedding</th>\n",
       "      <th id=\"T_d4a46_level0_col1\" class=\"col_heading level0 col1\" >Logistic_Regression</th>\n",
       "      <th id=\"T_d4a46_level0_col2\" class=\"col_heading level0 col2\" >Support_Vector_Machine</th>\n",
       "      <th id=\"T_d4a46_level0_col3\" class=\"col_heading level0 col3\" >Random_Forest</th>\n",
       "      <th id=\"T_d4a46_level0_col4\" class=\"col_heading level0 col4\" >Decision_Tree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d4a46_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d4a46_row0_col0\" class=\"data row0 col0\" >gpt_3</td>\n",
       "      <td id=\"T_d4a46_row0_col1\" class=\"data row0 col1\" >0.832000</td>\n",
       "      <td id=\"T_d4a46_row0_col2\" class=\"data row0 col2\" >0.804000</td>\n",
       "      <td id=\"T_d4a46_row0_col3\" class=\"data row0 col3\" >0.788000</td>\n",
       "      <td id=\"T_d4a46_row0_col4\" class=\"data row0 col4\" >0.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d4a46_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d4a46_row1_col0\" class=\"data row1 col0\" >mpnet</td>\n",
       "      <td id=\"T_d4a46_row1_col1\" class=\"data row1 col1\" >0.756000</td>\n",
       "      <td id=\"T_d4a46_row1_col2\" class=\"data row1 col2\" >0.764000</td>\n",
       "      <td id=\"T_d4a46_row1_col3\" class=\"data row1 col3\" >0.772000</td>\n",
       "      <td id=\"T_d4a46_row1_col4\" class=\"data row1 col4\" >0.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d4a46_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d4a46_row2_col0\" class=\"data row2 col0\" >word2vec</td>\n",
       "      <td id=\"T_d4a46_row2_col1\" class=\"data row2 col1\" >0.736000</td>\n",
       "      <td id=\"T_d4a46_row2_col2\" class=\"data row2 col2\" >0.756000</td>\n",
       "      <td id=\"T_d4a46_row2_col3\" class=\"data row2 col3\" >0.776000</td>\n",
       "      <td id=\"T_d4a46_row2_col4\" class=\"data row2 col4\" >0.672000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d4a46_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d4a46_row3_col0\" class=\"data row3 col0\" >glove</td>\n",
       "      <td id=\"T_d4a46_row3_col1\" class=\"data row3 col1\" >0.700000</td>\n",
       "      <td id=\"T_d4a46_row3_col2\" class=\"data row3 col2\" >0.748000</td>\n",
       "      <td id=\"T_d4a46_row3_col3\" class=\"data row3 col3\" >0.772000</td>\n",
       "      <td id=\"T_d4a46_row3_col4\" class=\"data row3 col4\" >0.684000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17615010d30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new key 'embeddings' to the dictionary 'accuracy_lists' and assign the list 'embedding_var' to it\n",
    "accuracy_lists['embeddings'] = embedding_var\n",
    "\n",
    "# Create a list of tuples using the values from the dictionaries\n",
    "df_zip = list(zip(accuracy_lists['embeddings'], accuracy_lists['lr'], accuracy_lists['svm'], accuracy_lists['rf'], accuracy_lists['dt']))\n",
    "\n",
    "# Create a DataFrame 'df_accuracy' from the list 'df_zip' and specify the column names\n",
    "df_accuracy = pd.DataFrame(df_zip, columns = ['Embedding','Logistic_Regression','Support_Vector_Machine', 'Random_Forest','Decision_Tree'])\n",
    "\n",
    "# Add a background gradient to the DataFrame for visual representation\n",
    "df_accuracy.style.background_gradient()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compu1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b326bab814914bb9362655fffb4210573395516aa7f0eb5182e2411c18758c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
